# @package _group_

# for tuning
hyperparameters:
  batch_size: 
    name: "batch_size"
    choices: [8, 16, 32]

  learning_rate: 
    name: "lr"
    lower: 0.00001
    upper: 0.005

  n_layers:
    name: "n_layers"
    lower: 3
    upper: 10
  
  n_hidden:
    name: "n_hidden"
    choices: [32, 64, 128, 256]

  n_hidden_lin:
    name: "n_hidden_lin"
    choices: [0, 32, 64, 128, 256]

  dropout:
    name: "dropout"
    lower: 0.0
    upper: 0.9
    step: 0.1

  dropout_gru:
    name: "dropout_gru"
    lower: 0.0
    upper: 0.9
    step: 0.1

  bidir:
    name: "bidir"
    choices: [False, True]

  tuned_fastt:
    name: "tuned_fastt"
    choice: [None, 1, 3, 5]


  


# for training
name: 'lstm'

unanimous:
  batch_size: 32
  bidir: false
  dropout_gru: 0.1
  lr: 0.00021749914884071396
  n_hidden: 128
  n_hidden_lin: 0
  n_layers: 4
  unit: GRU # GRU or LSTM units
  n_output: 3 # there are 3 output classes but it can make sense to add a 4th "rest" class
  n_epochs: 1000
  clip: 1. # for gradient clip to prevent exploding gradient problem in LSTM/RNN
  one_hot: False
  patience: 3
  model_path: ''
  seq_length: 26

majority:
  batch_size: 16
  bidir: true
  dropout_gru: 0.4
  lr: 0.0004470478831110828
  n_hidden: 256
  n_hidden_lin: 0
  n_layers: 3
  unit: GRU # GRU or LSTM units
  n_output: 3 # there are 3 output classes but it can make sense to add a 4th "rest" class
  n_epochs: 1000
  clip: 1. # for gradient clip to prevent exploding gradient problem in LSTM/RNN
  one_hot: False
  patience: 3
  model_path: models/other_models/GRU/dev-majority/model.pth
  seq_length: 26
