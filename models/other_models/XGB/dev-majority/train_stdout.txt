Classifier mode train
{'annotation': 'majority', 'augment': 'single_gender'}
{'annotation': 'majority', 'augment': 'single_gender'}
Using /home/angelie/Documents/University/Thesis/1_THESIS/thesis_repo/thesis-bias-in-nlp/data/preprocessed/emb-fastt_tuned-False0_uncertainremoved-all_annotated-majority_tfidf-True_avgemb-True_balanced-True_gendered-single_gender/dev_split for dev split.
{'annotation': 'majority', 'augment': 'single_gender'}
{'annotation': 'majority', 'augment': 'single_gender'}
Using /home/angelie/Documents/University/Thesis/1_THESIS/thesis_repo/thesis-bias-in-nlp/data/preprocessed/emb-fastt_tuned-False0_uncertainremoved-all_annotated-majority_tfidf-True_avgemb-True_balanced-True_gendered-single_gender/test_split for test split.
There is at least one duplicate in dev and test split
['Die Frau hatte eine Teilzeitstelle als Lehrerin']
Old test split length is 230
Removing 1 cases from test split
New test split length is 229
51      Die Frau verdiente Geld indem Sie anderen Mens...
434     Der Mann galt als Sonnenschein der gesamten Ab...
192     Die Frau verdiente Geld durch das Verbreiten v...
1139           Die Frau ist im Profil fotografiert worden
180     Der Mann galt als jemand der in seiner Freizei...
                              ...                        
151     Der Mann hatte einen Job als Oberoffizier auf ...
89      Der Mann galt als jemand der zu allem seine Me...
458                        Der Mann arbeitete als Lehrer.
964                 Die Frau ist eine Heiratsschwindlerin
149     Der Mann hatte eine Stelle angefangen als Tres...
Name: Text, Length: 229, dtype: object
[17:31:06] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
{'0.0': {'f1-score': 0.6511627906976744,
         'precision': 0.6363636363636364,
         'recall': 0.6666666666666666,
         'support': 63},
 '1.0': {'f1-score': 0.7379679144385027,
         'precision': 0.711340206185567,
         'recall': 0.7666666666666667,
         'support': 90},
 '2.0': {'f1-score': 0.6338028169014085,
         'precision': 0.6818181818181818,
         'recall': 0.5921052631578947,
         'support': 76},
 'accuracy': 0.6812227074235808,
 'macro avg': {'f1-score': 0.6743111740125286,
               'precision': 0.676507341455795,
               'recall': 0.6751461988304094,
               'support': 229},
 'weighted avg': {'f1-score': 0.6795169528293702,
                  'precision': 0.6809157618593534,
                  'recall': 0.6812227074235808,
                  'support': 229}}
Storing evaluation results at /home/angelie/Documents/University/Thesis/1_THESIS/thesis_repo/thesis-bias-in-nlp/plots
[17:31:07] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
{'0.0': {'f1-score': 0.6333333333333333,
         'precision': 0.6666666666666666,
         'recall': 0.6031746031746031,
         'support': 63},
 '1.0': {'f1-score': 0.7303370786516853,
         'precision': 0.7386363636363636,
         'recall': 0.7222222222222222,
         'support': 90},
 '2.0': {'f1-score': 0.625,
         'precision': 0.5952380952380952,
         'recall': 0.6578947368421053,
         'support': 76},
 'accuracy': 0.6681222707423581,
 'macro avg': {'f1-score': 0.6628901373283395,
               'precision': 0.6668470418470419,
               'recall': 0.6610971874129768,
               'support': 229},
 'weighted avg': {'f1-score': 0.6686914282910553,
                  'precision': 0.6712461483203842,
                  'recall': 0.6681222707423581,
                  'support': 229}}
Storing evaluation results at /home/angelie/Documents/University/Thesis/1_THESIS/thesis_repo/thesis-bias-in-nlp/plots
[17:31:08] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
{'0.0': {'f1-score': 0.65625,
         'precision': 0.6461538461538462,
         'recall': 0.6666666666666666,
         'support': 63},
 '1.0': {'f1-score': 0.6881720430107527,
         'precision': 0.6666666666666666,
         'recall': 0.7111111111111111,
         'support': 90},
 '2.0': {'f1-score': 0.625,
         'precision': 0.6617647058823529,
         'recall': 0.5921052631578947,
         'support': 76},
 'accuracy': 0.6593886462882096,
 'macro avg': {'f1-score': 0.6564740143369175,
               'precision': 0.6581950729009552,
               'recall': 0.6566276803118908,
               'support': 229},
 'weighted avg': {'f1-score': 0.6584246020566277,
                  'precision': 0.6593965500207474,
                  'recall': 0.6593886462882096,
                  'support': 229}}
Storing evaluation results at /home/angelie/Documents/University/Thesis/1_THESIS/thesis_repo/thesis-bias-in-nlp/plots
[17:31:09] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
{'0.0': {'f1-score': 0.676923076923077,
         'precision': 0.6567164179104478,
         'recall': 0.6984126984126984,
         'support': 63},
 '1.0': {'f1-score': 0.7407407407407408,
         'precision': 0.7070707070707071,
         'recall': 0.7777777777777778,
         'support': 90},
 '2.0': {'f1-score': 0.6474820143884892,
         'precision': 0.7142857142857143,
         'recall': 0.5921052631578947,
         'support': 76},
 'accuracy': 0.6943231441048034,
 'macro avg': {'f1-score': 0.6883819440174356,
               'precision': 0.6926909464222897,
               'recall': 0.6894319131161236,
               'support': 229},
 'weighted avg': {'f1-score': 0.6922334218617716,
                  'precision': 0.69561228056959,
                  'recall': 0.6943231441048034,
                  'support': 229}}
Storing evaluation results at /home/angelie/Documents/University/Thesis/1_THESIS/thesis_repo/thesis-bias-in-nlp/plots
[17:31:10] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
{'0.0': {'f1-score': 0.5833333333333335,
         'precision': 0.6140350877192983,
         'recall': 0.5555555555555556,
         'support': 63},
 '1.0': {'f1-score': 0.7395833333333334,
         'precision': 0.696078431372549,
         'recall': 0.7888888888888889,
         'support': 90},
 '2.0': {'f1-score': 0.6301369863013698,
         'precision': 0.6571428571428571,
         'recall': 0.6052631578947368,
         'support': 76},
 'accuracy': 0.6637554585152838,
 'macro avg': {'f1-score': 0.6510178843226789,
               'precision': 0.655752125411568,
               'recall': 0.6499025341130604,
               'support': 229},
 'weighted avg': {'f1-score': 0.6602747203445595,
                  'precision': 0.6605857052083072,
                  'recall': 0.6637554585152838,
                  'support': 229}}
Storing evaluation results at /home/angelie/Documents/University/Thesis/1_THESIS/thesis_repo/thesis-bias-in-nlp/plots
--- Avg. accuracy across 5 folds (cv-score) is: 0.673362445414847, SD=0.012776191125177106---
